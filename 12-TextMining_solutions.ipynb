{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab\n",
    "==========================================\n",
    "Text Feature Extraction for Classification\n",
    "------------------------------------------\n",
    "Alessandro D. Gagliardi  \n",
    "*(adapted from Olivier Grisel's tutorial)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<style>\n",
    "div.input {\n",
    "    width: 105ex; /* about 80 chars + buffer */\n",
    "}\n",
    "div.text_cell {\n",
    "    width: 105ex; /* instead of 100%, */\n",
    "}\n",
    "div.text_cell_render {\n",
    "    /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/\n",
    "    font-family: \"Charis SIL\", serif !important; /* Make non-code text serif. */\n",
    "    line-height: 145% !important; /* added for some line spacing of text. */\n",
    "    width: 105ex !important; /* instead of 'inherit' for shorter lines */\n",
    "}\n",
    "/* Set the size of the headers */\n",
    "div.text_cell_render h1 {\n",
    "    font-size: 18pt;\n",
    "}\n",
    "div.text_cell_render h2 {\n",
    "    font-size: 14pt;\n",
    "}\n",
    ".CodeMirror {\n",
    "     font-family: Consolas, monospace;\n",
    "}\n",
    "</style>\n",
    "\n",
    "Outline of this section:\n",
    "\n",
    "- Turn a corpus of text documents into **feature vectors** using a **Bag of Words** representation,\n",
    "- Train a simple text classifier on the feature vectors,\n",
    "- Wrap the vectorizer and the classifier with a **pipeline**,\n",
    "- Cross-validation and **model selection** on the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3d82390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from seaborn import plt\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Some nice default configuration for plots\n",
    "plt.rcParams['figure.figsize'] = 10, 7.5\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.gray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pairs\n",
    "=========\n",
    "\n",
    "```bash\n",
    "$ unzip Classification_data -d Classification_data\n",
    "```\n",
    "\n",
    "1. Load the dateset using `load_files` (hint: our `categories` are now `spam`, `easy_ham`, etc.)\n",
    "2. Write a pre-processor callable to remove the message headers.\n",
    "3. Set up a pipeline for cross validation and model selection using `spam` and `easy_ham`.\n",
    "    - Which parameters should be optimized?\n",
    "    - Do you expect the results to be different from the parameters above? Why/why not?\n",
    "    - Are there other parameters we should optimize that we haven't tested?\n",
    "4. Use `GridSearchCV` to find optimal parameters for vectorizor and classifier.\n",
    "5. Run classifier against `hard_ham`. What percentage of `hard_ham` does it correctly identify as not `spam`?\n",
    "6. Display the most discriminative features. Anything stick out?\n",
    "7. Run classifier against `spam_2`, `easy_ham_2`, `hard_ham_2`. \n",
    "    - Plot the ROC curve (along with AUC) for each case. \n",
    "    - Print the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Load the dateset using `load_files` (hint: our `categories` are now `spam`, `easy_ham`, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://liftopia.box.com/s/vbevlwnt2ct8iapb1uoc5nmq5axjyu94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = [\n",
    "    'spam',\n",
    "    'easy_ham', \n",
    "]\n",
    "\n",
    "train_data = load_files('datasets/no_hard_ham_train/',categories=categories, encoding='latin-1')\n",
    "test_data = load_files('datasets/no_hard_ham_test/',categories=categories, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lets make sure we have the data:\n",
    "\n",
    "all_target_names = train_data.target_names\n",
    "\n",
    "def display_sample(i, dataset):\n",
    "    print(\"Class name: \" + dataset.target_names[dataset.target[i]])\n",
    "    print(\"Text content:\\n\")\n",
    "    print(dataset.data[i])\n",
    "\n",
    "    \n",
    "#display_sample(0, train_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Write a pre-processor callable to remove the message headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def strip_headers(post):\n",
    "    \"\"\"Find the first blank line and drop the headers to keep the body\"\"\"\n",
    "    if '\\n\\n' in post:\n",
    "        headers, body = post.split('\\n\\n', 1)\n",
    "        return body.lower()\n",
    "    else:\n",
    "        # Unexpected post inner-structure, be conservative\n",
    "        # and keep everything\n",
    "        return post.lower()\n",
    "    \n",
    "#print strip_headers(train_data.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Set up a pipeline for cross validation and model selection using `spam` and `easy_ham`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#    - Which parameters should be optimized?\n",
    "#    - Do you expect the results to be different from the parameters above? Why/why not?\n",
    "#    - Are there other parameters we should optimize that we haven't tested?\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "strip_vectorizer = TfidfVectorizer(preprocessor=strip_headers, min_df=2)\n",
    "\n",
    "pipeline = Pipeline((\n",
    "    ('vec', strip_vectorizer),\n",
    "    ('clf', MultinomialNB()),\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. Use `GridSearchCV` to find optimal parameters for vectorizor and classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'vec__max_df': [0.8, 1.0],\n",
    "    'vec__ngram_range': [(1, 1), (1, 2)],\n",
    "    'clf__alpha': np.logspace(-5, 0, 6)\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline, parameters, verbose=0, refit=False, n_jobs=3)\n",
    "_ = gs.fit(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99428367274026441"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 1.0000000000000001e-05,\n",
       " 'vec__max_df': 0.8,\n",
       " 'vec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#build a model using these \"ideal params\"\n",
    "\n",
    "pipeline = Pipeline((\n",
    "    ('vec', TfidfVectorizer(max_df = 0.8, ngram_range = (1, 2), use_idf=True, preprocessor=strip_headers)),\n",
    "    ('clf', MultinomialNB(alpha = 0.00001)),\n",
    "))\n",
    "\n",
    "\n",
    "modelNB = pipeline.fit(train_data.data, train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94569489103251159"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the model against the training data\n",
    "modelNB.score(test_data.data, test_data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5. Run classifier against `hard_ham`. What percentage of `hard_ham` does it correctly identify as not `spam`?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load hard ham data\n",
    "categories2 = ['hard_ham']\n",
    "test_hard_ham = load_files('datasets/with_hard_ham_train/',categories=categories2, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['easy_ham', 'spam']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td> 0</td>\n",
       "      <td> 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   actual  predicted\n",
       "0       0          0\n",
       "1       0          1\n",
       "2       0          1\n",
       "3       0          1\n",
       "4       0          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show the values in a dataframe\n",
    "predicted = modelNB.predict(test_hard_ham.data)\n",
    "actual = test_hard_ham.target\n",
    "df = pd.DataFrame(actual, columns = ['actual'])\n",
    "df['predicted'] = predicted\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#calculate number of times when it predicts hard ham as spam / total observations\n",
    "predicted_as_spam = float(sum(predicted))/len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33999999999999997"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subtract from 1 to find the % of occurrences where hard_ham is correctly identified as not spam. \n",
    "1-predicted_as_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##7. Run classifier against `spam_2`, `easy_ham_2`, `hard_ham_2`. \n",
    "   - Plot the ROC curve (along with AUC) for each case. \n",
    "    - Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reloading the data with all three categories\n",
    "\n",
    "categories = [\n",
    "    'spam',\n",
    "    'hard_ham',\n",
    "    'easy_ham', \n",
    "]\n",
    "\n",
    "train_data2 = load_files('datasets/with_hard_ham_train/',categories=categories, encoding='latin-1')\n",
    "test_data2 = load_files('datasets/with_hard_ham_test/',categories=categories, encoding='latin-1')\n",
    "\n",
    "pipeline = Pipeline((\n",
    "    ('vec', TfidfVectorizer(max_df = 0.8, ngram_range = (1, 2), use_idf=True, preprocessor=strip_headers)),\n",
    "    ('clf', MultinomialNB(alpha = 0.00001)),\n",
    "))\n",
    "\n",
    "modelNB = pipeline.fit(train_data2.data, train_data2.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91532655070561209"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how accurate is this model now that we included hard_ham?\n",
    "modelNB.score(test_data2.data, test_data2.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"3\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>easy_ham</th>\n",
       "      <th>hard_ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">actual</th>\n",
       "      <th>easy_ham</th>\n",
       "      <td> 1384</td>\n",
       "      <td>  14</td>\n",
       "      <td>    3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard_ham</th>\n",
       "      <td>    0</td>\n",
       "      <td> 248</td>\n",
       "      <td>    0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>  116</td>\n",
       "      <td> 125</td>\n",
       "      <td> 1157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 predicted                \n",
       "                  easy_ham  hard_ham  spam\n",
       "actual easy_ham       1384        14     3\n",
       "       hard_ham          0       248     0\n",
       "       spam            116       125  1157"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predicted = pipeline.predict(test_data2.data)\n",
    "\n",
    "pd.DataFrame(confusion_matrix(test_data2.target, predicted), \n",
    "             index = pd.MultiIndex.from_product([['actual'], test_data2.target_names]),\n",
    "             columns = pd.MultiIndex.from_product([['predicted'], test_data2.target_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(target_test, target_predicted_proba, categories):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from sklearn.metrics import auc\n",
    "    \n",
    "    for pos_label, category in enumerate(categories):\n",
    "        fpr, tpr, thresholds = roc_curve(target_test, target_predicted_proba[:, pos_label], pos_label)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label='{} ROC curve (area = {:.3f})'.format(category, roc_auc))\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "    plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'modelNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a2f5b7676438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget_predicted_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelNB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_roc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_predicted_proba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_spam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'modelNB' is not defined"
     ]
    }
   ],
   "source": [
    "target_predicted_proba = modelNB.predict_proba(test_data2.data)\n",
    "plot_roc_curve(y_train, target_predicted_proba, is_spam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On Your Own\n",
    "========\n",
    "\n",
    "1. Read the Naïve Bayes documentation at [scikit-learn.org](http://scikit-learn.org/stable/modules/naive_bayes.html). There are three Naïve Bayes classifiers described. Which of the other two might also be appropriate for this task?\n",
    "2. Explain your choice and apply it to either the spam/ham dataset (if you completed the pair assignment) or the newsgroups dataset (if you didn't).\n",
    "3. Use grid search cross validation to find the best parameters for both the vectorizor and classifier.\n",
    "    - Do different parameters for the vectorizor work better for this classifier?\n",
    "4. Does this classifier do better or worse than the multinomial classifier?\n",
    "5. Advanced: consider the descriptions of the two classifiers in light of which does better for this problem. Can you posit a theory as to why one classifier should do better than the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##1. Read the Naïve Bayes documentation at [scikit-learn.org](http://scikit-learn.org/stable/modules/naive_bayes.html). There are three Naïve Bayes classifiers described. Which of the other two might also be appropriate for this task?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Bernouli\n",
    "# From the docs:\n",
    "# \"In the case of text classification, word occurrence vectors (rather than word count vectors) \n",
    "# may be used to train and use this classifier. BernoulliNB might perform better on some datasets,\n",
    "# especially those with shorter documents. It is advisable to evaluate both models, if time permits.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Explain your choice and apply it to either the spam/ham dataset (if you completed the pair assignment) or the newsgroups dataset (if you didn't).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#re-using the data from above, with all three categories included\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "pipeline2 = Pipeline((\n",
    "    ('vec', TfidfVectorizer(preprocessor=strip_headers)),\n",
    "    ('clf', BernoulliNB(binarize=0.0)),\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##3. Use grid search cross validation to find the best parameters for both the vectorizor and classifier.\n",
    "    - Do different parameters for the vectorizor work better for this classifier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'vec__max_df': [0.8, 1.0],\n",
    "    'vec__ngram_range': [(1, 1), (1,2)],\n",
    "    'clf__alpha': np.logspace(-5, 0, 6)\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipeline2, parameters, verbose=0, refit=False, n_jobs=1)\n",
    "_ = gs.fit(train_data2.data, train_data2.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9458794587945879"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__alpha': 1.0000000000000001e-05,\n",
       " 'vec__max_df': 0.8,\n",
       " 'vec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##4. Does this classifier do better or worse than the multinomial classifier?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#fit the bernoulli model with the ideal params\n",
    "#using threshold of 0.0 to binarize features\n",
    "\n",
    "\n",
    "pipeline2 = Pipeline((\n",
    "    ('vec', TfidfVectorizer(max_df = 0.8, ngram_range = (1, 2), use_idf=True, preprocessor=strip_headers)),\n",
    "    ('clf', BernoulliNB(alpha = 0.00001, binarize = 0.0)),\n",
    "))\n",
    "\n",
    "modelBB = pipeline2.fit(train_data2.data, train_data2.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8592057761732852"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try it against the test data\n",
    "\n",
    "\n",
    "modelBB.score(test_data2.data, test_data2.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##5. Advanced: consider the descriptions of the two classifiers in light of which does better for this problem. Can you posit a theory as to why one classifier should do better than the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We've lost some of our data by \"binarizing\" the vectors, so the model is less accurate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
